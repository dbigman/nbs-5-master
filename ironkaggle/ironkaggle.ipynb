{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data, EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Importing data into df\n",
    "file_path = 'sales.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "ironkaggle_df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "# print(ironkaggle_df.head())\n",
    "\n",
    "print(ironkaggle_df.dtypes)\n",
    "# print(ironkaggle_df.describe())\n",
    "print(ironkaggle_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <!-- | **Variable**           | **Type**         | **Details**                              |\n",
    "# |-------------------------|------------------|------------------------------------------|\n",
    "# | `store_ID`             | Categorical     | Nominal, unique store identifiers        |\n",
    "# | `day_of_week`          | Categorical     | Represents days of the week   |\n",
    "# | `date`                 | Ordinal         | Nominal (date) |\n",
    "# | `nb_customers_on_day`  | Numerical       | number of customers          |\n",
    "# | `open`                 | Categorical     | Binary, open or closed         |\n",
    "# | `promotion`            | Categorical     | Binary, promotion status                 |\n",
    "# | `state_holiday`        | Categorical     | Binary, holiday status                  |\n",
    "# | `school_holiday`       | Categorical     | Binary, holiday status                   |\n",
    "# | `sales`                | Numerical       | Continuous, sales revenue                | \n",
    "\n",
    "categorical_var = ['store_ID', 'day_of_week', 'open', 'promotion', 'state_holiday', 'school_holiday']\n",
    "date_var = ['date']\n",
    "numerical_var = ['nb_customers_on_day', 'sales']\n",
    "\n",
    "# I will drop the untitled column, bc I don't know what it is. \n",
    "ironkaggle_df = ironkaggle_df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Converting the 'date' column to datetime\n",
    "ironkaggle_df['date'] = pd.to_datetime(ironkaggle_df['date'])\n",
    "\n",
    "# Count the number of unique values in a column\n",
    "unique_values = ironkaggle_df['state_holiday'].unique()\n",
    "print(f\"Unique values in the column: {unique_values}\")\n",
    " \n",
    "# Unique values in the column: ['0' 'a' 'c' 'b']\n",
    "# Converting 'state_holiday' ['0' '1' '2 '3']. \n",
    "\n",
    "# Define the mapping\n",
    "mapping = {'0': '0', 'a': '1', 'c': '2', 'b': '3'}\n",
    "\n",
    "# Apply the mapping to the column\n",
    "ironkaggle_df['state_holiday'] = ironkaggle_df['state_holiday'].map(mapping)\n",
    "# converting to int\n",
    "ironkaggle_df['state_holiday'] = ironkaggle_df['state_holiday'].astype('int64')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical var\n",
    "\n",
    "for col in categorical_var:\n",
    "    unique_count = ironkaggle_df[col].nunique()\n",
    "    print(f\"The column '{col}' has {unique_count} unique values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'open' is 0\n",
    "closed_sales = ironkaggle_df[ironkaggle_df['open'] == 0]['sales']\n",
    "\n",
    "# Summary statistics for 'sales' when 'open' is 0\n",
    "closed_sales_summary = closed_sales.describe()\n",
    "# print(closed_sales_summary)\n",
    "\n",
    "# There are no sales when stores are closed. These rows do not provide insights for sales analysis\n",
    "# I will remove them from the dataset. \n",
    "\n",
    "# Remove rows where 'open' is 0 and 'sales' is also 0\n",
    "ironkaggle_df_filtered = ironkaggle_df[~((ironkaggle_df['open'] == 0) & (ironkaggle_df['sales'] == 0))]\n",
    "# Drop the entire 'open' feature\n",
    "ironkaggle_df_filtered = ironkaggle_df_filtered.drop(columns=['open'])\n",
    "categorical_var.remove('open')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the numerical var, what are their descriptive statistics?\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "def calculate_descriptive_stats(df, column):\n",
    "    \"\"\"\n",
    "    Calculates descriptive statistics for a given column of a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        column (str): The column name for which to calculate statistics.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the calculated statistics.\n",
    "    \"\"\"\n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        \"Statistic\": [\n",
    "            \"Mean\",\n",
    "            \"Median\",\n",
    "            \"Mode\",\n",
    "            \"Range\",\n",
    "            \"Variance\",\n",
    "            \"Standard Deviation\",\n",
    "            \"Interquartile Range (IQR)\",\n",
    "            \"Skewness\",\n",
    "            \"Kurtosis\",\n",
    "            \"Minimum\",\n",
    "            \"Maximum\",\n",
    "            \"Sum\",\n",
    "            \"Count\",\n",
    "            \"25th Percentile\",\n",
    "            \"75th Percentile\",\n",
    "        ],\n",
    "        \"Value\": [\n",
    "            df[column].mean(),\n",
    "            df[column].median(),\n",
    "            df[column].mode()[0] if not df[column].mode().empty else np.nan,\n",
    "            df[column].max() - df[column].min(),\n",
    "            df[column].var(),\n",
    "            df[column].std(),\n",
    "            df[column].quantile(0.75) - df[column].quantile(0.25),\n",
    "            df[column].skew(),\n",
    "            df[column].kurt(),\n",
    "            df[column].min(),\n",
    "            df[column].max(),\n",
    "            df[column].sum(),\n",
    "            df[column].count(),\n",
    "            df[column].quantile(0.25),\n",
    "            df[column].quantile(0.75),\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame\n",
    "    stats_df = pd.DataFrame(stats)\n",
    "\n",
    "    # Print neatly using tabulate\n",
    "    # print(tabulate(stats_df, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "    return stats_df\n",
    "\n",
    "def calculate_stats_for_multiple_columns(df, columns):\n",
    "    \"\"\"\n",
    "    Calculates descriptive statistics for multiple columns and stores them in a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        columns (list): List of column names to calculate statistics for.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are column names and values are DataFrames of statistics.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for column in columns:\n",
    "        # print(f\"\\nDescriptive Statistics for {column}:\")\n",
    "        stats_df = calculate_descriptive_stats(df, column)\n",
    "        results[column] = stats_df\n",
    "    return results\n",
    "\n",
    "# Example Usage\n",
    "# Assuming `df` is your DataFrame and `numerical_var` is a list of numerical columns\n",
    "numerical_var = ['nb_customers_on_day', 'sales']\n",
    "results = calculate_stats_for_multiple_columns(ironkaggle_df_filtered, numerical_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping by store\n",
    "\n",
    "# Group 'sales' by 'store_ID' and compute aggregate statistics\n",
    "grouped_sales = ironkaggle_df_filtered.groupby('store_ID')['sales'].agg(['sum', 'mean', 'median', 'count', 'max', 'min']).reset_index()\n",
    "\n",
    "# Display the grouped DataFrame\n",
    "# print(grouped_sales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot sum and means to see distributions\n",
    "# Extracting data for plotting\n",
    "store_count = len(grouped_sales[\"store_ID\"])\n",
    "sums = grouped_sales[\"sum\"]\n",
    "mean = grouped_sales[\"mean\"]\n",
    "\n",
    "# Convert sums to millions and means to thousands\n",
    "sums_in_millions = [x / 1_000_000 for x in sums]\n",
    "means_in_thousands = [x / 1_000 for x in mean]\n",
    "\n",
    "# Create a single figure for all 4 plots\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Histogram for sum of sales\n",
    "plt.subplot(2, 2, 1)\n",
    "n, bins, patches = plt.hist(sums_in_millions, bins=10, alpha=0.8, edgecolor='black')\n",
    "for i in range(len(n)):\n",
    "    plt.text((bins[i] + bins[i + 1]) / 2, n[i], str(int(n[i])), ha='center', va='bottom')\n",
    "plt.title(\"Histogram of Sum of Sales (in Millions)\")\n",
    "plt.xlabel(\"Sum of Sales (in Millions)\")\n",
    "plt.ylabel(\"Store Count\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Histogram for mean sales\n",
    "plt.subplot(2, 2, 2)\n",
    "n, bins, patches = plt.hist(means_in_thousands, bins=10, alpha=0.8, edgecolor='black', color='orange')\n",
    "for i in range(len(n)):\n",
    "    plt.text((bins[i] + bins[i + 1]) / 2, n[i], str(int(n[i])), ha='center', va='bottom')\n",
    "plt.title(\"Histogram of Mean Sales (in Thousands)\")\n",
    "plt.xlabel(\"Mean Sales (in Thousands)\")\n",
    "plt.ylabel(\"Store Count\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Box plot for sum of sales\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.boxplot(sums, vert=False, patch_artist=True, boxprops=dict(facecolor='lightblue'))\n",
    "plt.title(\"Box Plot of Sum of Sales\")\n",
    "plt.xlabel(\"Sum of Sales (in Dollars)\")\n",
    "\n",
    "# Box plot for mean of sales\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.boxplot(mean, vert=False, patch_artist=True, boxprops=dict(facecolor='lightgreen'))\n",
    "plt.title(\"Box Plot of Mean Sales\")\n",
    "plt.xlabel(\"Mean Sales (in Dollars)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical var\n",
    "\n",
    "for col in categorical_var:\n",
    "    unique_count = ironkaggle_df_filtered[col].nunique()\n",
    "    print(f\"The column '{col}' has {unique_count} unique values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "correlation_matrix = ironkaggle_df_filtered.select_dtypes(include=['float64', 'int64']).corr()\n",
    "\n",
    "# SVisualize the correlation matrix using a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=True,        # Display correlation values on the heatmap\n",
    "    cmap=\"coolwarm\",   # Color map for visualization\n",
    "    fmt=\".2f\",         # Format to 2 decimal places\n",
    "    linewidths=0.5     # Add space between cells\n",
    ")\n",
    "plt.title(\"Correlation Matrix Heatmap for numerical var\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High Correlation Between nb_customers_on_day and sales (0.82): strong positive linear relationship. It suggests that an increase in the number of customers on a day is strongly associated with an increase in sales. \n",
    "Moderate Correlation Between promotion and sales (0.37): positive relationship, albeit weaker than with customer numbers. This implies that promotions have a noticeable, but not overwhelming, impact on sales.\n",
    "\n",
    "Weak Correlations with school_holiday: school_holiday shows very weak correlations with other variables, including sales (0.04). This suggests that school holidays might not significantly impact sales or customer behavior in this dataset.\n",
    "\n",
    "Negative Correlation Between day_of_week and promotion/sales (-0.29 and -0.18):  negative correlation with day_of_week suggests that certain days of the week might have fewer promotions or lower sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droppng 'school_holiday' bc very weak corr.\n",
    "ironkaggle_df_filtered = ironkaggle_df_filtered.drop(columns=['school_holiday'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = 'sales'\n",
    "\n",
    "charges_mean = round(ironkaggle_df_filtered[target_variable].mean(), 2)\n",
    "print(f\"The mean of our target variable '{target_variable}' is {charges_mean}\")\n",
    "\n",
    "# Establishing a naive baseline in regression provides a simple benchmark for model performance. \n",
    "# It helps identify if a model is adding value by exagerating trivial predictions, showing overfitting or underfitting. \n",
    "# Without a baseline, it's difficult to evaluate MSE or R2 and determine if a model is effective or shoudl be improved.\n",
    "\n",
    "# Also, I will convert date to numerical features.\n",
    "ironkaggle_df_filtered['year'] = ironkaggle_df_filtered['date'].dt.year\n",
    "ironkaggle_df_filtered['month'] = ironkaggle_df_filtered['date'].dt.month\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Modelling Without GridSearch or Pipeline\n",
    "\n",
    "Let's build a simple linear regression model without any feature engineering, grid search, or pipeline. This will serve as our initial baseline for comparison.\n",
    "\n",
    "### Task:\n",
    "- Split the data into training and test sets\n",
    "- Train a simple linear regression model\n",
    "- Evaluate its performance using regression metrics\n",
    "- Write it down as a markdown below so you can keep track. This is a scientific experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "### LOADING INFORMATION ###\n",
    "\n",
    "# Creating a copy of the DataFrame\n",
    "ironkaggle_df_filtered_copy = ironkaggle_df_filtered.copy()\n",
    "\n",
    "# # One-hot encoding for categorical variables\n",
    "# insurance_df_copy_encoded = pd.get_dummies(insurance_df_copy, drop_first=True)\n",
    "\n",
    "################################################################\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through all columns in the DataFrame except the target variable and 'date'\n",
    "for feature in ironkaggle_df_filtered_copy.columns:\n",
    "    # Skip the target variable or 'date' as in the datetime date\n",
    "    if feature == target_variable or feature == 'date':\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nPerforming Linear Regression for Feature: {feature}\")\n",
    "    \n",
    "    # Define the feature (X) as a single column\n",
    "    X = ironkaggle_df_filtered_copy[[feature]]\n",
    "    y = ironkaggle_df_filtered_copy[[target_variable]]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize and train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Append the results for the current feature\n",
    "    results.append([feature, mse, r2])\n",
    "    \n",
    "    # Visualize the regression line\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X, y, color=\"blue\", label=\"Actual Data\")\n",
    "    plt.plot(X, model.predict(X), color=\"red\", label=\"Regression Line\")\n",
    "    plt.title(f\"Linear Regression: {target_variable} vs {feature}\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(target_variable)\n",
    "    plt.legend()\n",
    "    plt.grid(linestyle=\"--\", alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "# Create and print a table with the results\n",
    "headers = [\"Feature\", \"Mean Squared Error (MSE)\", \"R-squared (R2)\"]\n",
    "print(\"\\nSummary of Results:\")\n",
    "print(tabulate(results, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nb_customers_on_day has the lowest MSE (3.09e+06), meaning it's the best-performing feature.\n",
    "Features like store_ID, state_holiday, and year have high MSEs (close to 9.6e+06), indicating they contribute little to reducing prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping 'store_ID' and 'state_holiday' to improve model performance and rerunning the model\n",
    "\n",
    "# Creating a copy of the DataFrame\n",
    "ironkaggle_df_re_filtered_copy = ironkaggle_df_filtered.copy()\n",
    "\n",
    "ironkaggle_df_re_filtered_copy = ironkaggle_df_re_filtered_copy.drop(columns=['store_ID', 'state_holiday'])\n",
    "\n",
    "### LOADING INFORMATION ###\n",
    "\n",
    "\n",
    "################################################################\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through all columns in the DataFrame except the target variable and 'date'\n",
    "for feature in ironkaggle_df_re_filtered_copy.columns:\n",
    "    # Skip the target variable or 'date' as in the datetime date\n",
    "    if feature == target_variable or feature == 'date':\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nPerforming Linear Regression for Feature: {feature}\")\n",
    "    \n",
    "    # Define the feature (X) as a single column\n",
    "    X = ironkaggle_df_re_filtered_copy[[feature]]\n",
    "    y = ironkaggle_df_re_filtered_copy[[target_variable]]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize and train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Append the results for the current feature\n",
    "    results.append([feature, mse, r2])\n",
    "    \n",
    "    # Visualize the regression line\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X, y, color=\"blue\", label=\"Actual Data\")\n",
    "    plt.plot(X, model.predict(X), color=\"red\", label=\"Regression Line\")\n",
    "    plt.title(f\"Linear Regression: {target_variable} vs {feature}\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(target_variable)\n",
    "    plt.legend()\n",
    "    plt.grid(linestyle=\"--\", alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "# Create and print a table with the results\n",
    "headers = [\"Feature\", \"Mean Squared Error (MSE)\", \"R-squared (R2)\"]\n",
    "print(\"\\nSummary of Results:\")\n",
    "print(tabulate(results, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature\tAnalysis\n",
    "day_of_week\tWeak predictor (R² = 0.032). There may be small weekly trends, but it adds minimal value.\n",
    "nb_customers_on_day\tStrongest predictor (R² = 0.678). Explains most of the variance in the target. This feature is essential.\n",
    "promotion\tModerate predictor (R² = 0.137). Indicates that promotions positively impact the target variable (likely sales).\n",
    "year\tVery weak predictor (R² = 0.0013). Likely irrelevant unless there are long-term trends.\n",
    "month\tWeak predictor (R² = 0.0059). Minimal seasonal or monthly impact.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling with Pipeline and Grid Search\n",
    "\n",
    "Now, let's see how using pipelines can simplify our workflow and prevent data leakage. We'll also use GridSearchCV to find the best hyperparameters.\n",
    "\n",
    "### Task:\n",
    "- Create a pipeline that includes scaling and linear regression\n",
    "- Define a parameter grid for hyperparameter tuning\n",
    "- Use GridSearchCV to find the best parameters and evaluate the model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# Define the features (X) and target (y)\n",
    "X = ironkaggle_df_re_filtered_copy.drop(columns=[target_variable, 'date'])\n",
    "y = ironkaggle_df_re_filtered_copy[target_variable]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),       # Scale features\n",
    "    ('regressor', LinearRegression())   # Apply Linear Regression\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"LinearRegression, Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"LinearRegression, R-squared (R2): {r2:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Another Model with Pipeline\n",
    "\n",
    "Let's try using a Gradient Boosting Regressor to see if it performs better.\n",
    "\n",
    "### Task:\n",
    "- Create and use a pipeline for Gradient Boosting Regressor\n",
    "- Define a parameter grid for grid search\n",
    "- Use GridSearchCV to find the best parameters and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define the features (X) and target (y)\n",
    "X = ironkaggle_df_re_filtered_copy.drop(columns=[target_variable, 'date'])\n",
    "y = ironkaggle_df_re_filtered_copy[target_variable]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),                 # Scale features\n",
    "    ('regressor', GradientBoostingRegressor(      # Gradient Boosting\n",
    "        n_estimators=100,                         # Hyperparameter: Number of boosting stages\n",
    "        learning_rate=0.1,                        # Hyperparameter: Learning rate\n",
    "        max_depth=3,                              # Hyperparameter: Maximum depth of trees\n",
    "        random_state=42                           # For reproducibility\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Gradient Boosting Regressor, Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Gradient Boosting Regressor, R-squared (R2): {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Wasn't able to do this, was like 20 minutes. \n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# # Define the features (X) and target (y)\n",
    "# X = ironkaggle_df_re_filtered_copy.drop(columns=[target_variable, 'date'])\n",
    "# y = ironkaggle_df_re_filtered_copy[target_variable]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define models and their parameter grids\n",
    "# models_and_parameters = [\n",
    "#     {\n",
    "#         'model': [LinearRegression()],\n",
    "#         'model__fit_intercept': [True, False]\n",
    "#     },\n",
    "#     {\n",
    "#         'model': [GradientBoostingRegressor()],\n",
    "#         'model__n_estimators': [100, 200],\n",
    "#         'model__learning_rate': [0.05, 0.1],\n",
    "#         'model__max_depth': [3, 5]\n",
    "#     },\n",
    "#     {\n",
    "#         'model': [RandomForestRegressor()],\n",
    "#         'model__n_estimators': [100, 200],\n",
    "#         'model__max_depth': [None, 10],\n",
    "#         'model__min_samples_split': [2, 5]\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# # Create a pipeline\n",
    "# pipeline = Pipeline([\n",
    "#     ('scaler', StandardScaler()),  # Step 1: Standardize the data\n",
    "#     ('model', LinearRegression())  \n",
    "# ])\n",
    "\n",
    "# # Perform GridSearchCV\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=pipeline, \n",
    "#     param_grid=models_and_parameters, \n",
    "#     cv=5,  # n-fold cross-validation\n",
    "#     scoring='r2', \n",
    "#     n_jobs=-1 \n",
    "# )\n",
    "\n",
    "# # Fit the grid search\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best model and parameters\n",
    "# best_model = grid_search.best_estimator_\n",
    "# best_params = grid_search.best_params_\n",
    "\n",
    "# # Evaluate the best model on the test set\n",
    "# y_pred = best_model.predict(X_test)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(\"Best Model:\", best_model)\n",
    "# print(\"Best Parameters:\", best_params)\n",
    "# print(f\"Test Set Mean Squared Error (MSE): {mse:.2f}\")\n",
    "# print(f\"Test Set R-squared (R2): {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Xgboost\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# Define the features (X) and target (y)\n",
    "X = ironkaggle_df_re_filtered_copy.drop(columns=[target_variable, 'date'])\n",
    "y = ironkaggle_df_re_filtered_copy[target_variable]\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Optional: Standardize the features for better model performance\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the XGBoost regressor\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=100,      # Number of trees\n",
    "    learning_rate=0.1,     # Step size shrinkage\n",
    "    max_depth=6,           # Maximum tree depth\n",
    "    random_state=42,       # Reproducibility\n",
    "    objective='reg:squarederror'  # Regression loss function\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared (R²): {r2:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store_ID               int64\n",
      "day_of_week            int64\n",
      "nb_customers_on_day    int64\n",
      "promotion              int64\n",
      "year                   int32\n",
      "month                  int32\n",
      "day                    int32\n",
      "dtype: object\n",
      "['store_ID', 'day_of_week', 'nb_customers_on_day', 'promotion', 'year', 'month', 'day']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Feature shape mismatch, expected: 5, got 7",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(ironkaggle_real_df\u001b[38;5;241m.\u001b[39mdtypes)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(ironkaggle_real_df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m---> 66\u001b[0m sales_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mironkaggle_real_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py:1186\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[1;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_use_inplace_predict():\n\u001b[0;32m   1185\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1186\u001b[0m         predts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1187\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1188\u001b[0m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmargin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1194\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_alike(predts):\n\u001b[0;32m   1195\u001b[0m             \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:2524\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[1;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[0;32m   2520\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   2521\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`shape` attribute is required when `validate_features` is True.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2522\u001b[0m         )\n\u001b[0;32m   2523\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features() \u001b[38;5;241m!=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m-> 2524\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2525\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature shape mismatch, expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2526\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2527\u001b[0m         )\n\u001b[0;32m   2529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_np_array_like(data):\n\u001b[0;32m   2530\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _ensure_np_dtype\n",
      "\u001b[1;31mValueError\u001b[0m: Feature shape mismatch, expected: 5, got 7"
     ]
    }
   ],
   "source": [
    "# use for prediction\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Importing data into df\n",
    "file_path = 'REAL_DATA_PROCESSED.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "ironkaggle_real_df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "categorical_var = ['store_ID', 'day_of_week', 'open', 'promotion', 'state_holiday', 'school_holiday']\n",
    "date_var = ['date']\n",
    "numerical_var = ['nb_customers_on_day', 'sales']\n",
    "\n",
    "# I will drop the untitled column, bc I don't know what it is. \n",
    "ironkaggle_real_df = ironkaggle_real_df.drop(columns=['index'])\n",
    "\n",
    "# Converting the 'date' column to datetime\n",
    "ironkaggle_real_df['date'] = pd.to_datetime(ironkaggle_real_df['date'], dayfirst=True)\n",
    "\n",
    "# Count the number of unique values in a column\n",
    "unique_values = ironkaggle_real_df['state_holiday'].unique()\n",
    "# print(f\"Unique values in the column: {unique_values}\")\n",
    " \n",
    "# Unique values in the column: ['0' 'a' 'c' 'b']\n",
    "# Converting 'state_holiday' ['0' '1' '2 '3']. \n",
    "\n",
    "# Define the mapping\n",
    "mapping = {'0': '0', 'a': '1', 'c': '2', 'b': '3'}\n",
    "\n",
    "ironkaggle_real_df['year'] = ironkaggle_real_df['date'].dt.year\n",
    "ironkaggle_real_df['month'] = ironkaggle_real_df['date'].dt.month\n",
    "ironkaggle_real_df['day'] = ironkaggle_real_df['date'].dt.day\n",
    "\n",
    "ironkaggle_real_df = ironkaggle_real_df.drop(columns=['date'])\n",
    "\n",
    "# Apply the mapping to the column\n",
    "ironkaggle_real_df['state_holiday'] = ironkaggle_real_df['state_holiday'].map(mapping)\n",
    "\n",
    "# converting to int\n",
    "ironkaggle_real_df['state_holiday'] = ironkaggle_real_df['state_holiday'].astype('int64')\n",
    "\n",
    "# Remove rows where 'open' is 0 \n",
    "ironkaggle_real_df = ironkaggle_real_df[((ironkaggle_real_df['open'] == 0)) ]\n",
    "                                       \n",
    "# Drop the entire 'open' feature\n",
    "ironkaggle_real_df = ironkaggle_real_df.drop(columns=['open'])\n",
    "categorical_var.remove('open')\n",
    "\n",
    "# Drop the entire 'state_holiday' feature\n",
    "ironkaggle_real_df = ironkaggle_real_df.drop(columns=['state_holiday'])\n",
    "categorical_var.remove('state_holiday')\n",
    "\n",
    "# Drop the entire 'school_holiday' feature\n",
    "ironkaggle_real_df = ironkaggle_real_df.drop(columns=['school_holiday'])\n",
    "categorical_var.remove('school_holiday')\n",
    "\n",
    "# # Drop the entire 'day' feature\n",
    "# ironkaggle_real_df = ironkaggle_real_df.drop(columns=['day'])\n",
    "# categorical_var.remove('day')\n",
    "\n",
    "print(ironkaggle_real_df.dtypes)\n",
    "print(ironkaggle_real_df.columns.tolist())\n",
    "\n",
    "sales_predictions = model.predict(ironkaggle_real_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Neural Network\n",
    "\n",
    "To build the neural network, you can refer to your own codes you wrote while following the [Deep Learning with Python, TensorFlow, and Keras tutorial](https://www.youtube.com/watch?v=wQ8BIBpya2k) in the lesson. It's pretty similar to what you will be doing in this lab.\n",
    "\n",
    "1. Split the training and test data.\n",
    "1. Create a `Sequential` model.\n",
    "1. Add several layers to your model. Make sure you use ReLU as the activation function for the middle layers. Use Softmax for the output layer because each output has a single lable and all the label probabilities add up to 1.\n",
    "1. Compile the model using `adam` as the optimizer and `sparse_categorical_crossentropy` as the loss function. For metrics, use `accuracy` for now.\n",
    "1. Fit the training data.\n",
    "1. Evaluate your neural network model with the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tabulate import tabulate\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "# # Define the features (X) and target (y)\n",
    "# X = ironkaggle_df_re_filtered_copy.drop(columns=[target_variable, 'date'])\n",
    "# y = ironkaggle_df_re_filtered_copy[target_variable]\n",
    "\n",
    "\n",
    "# # Split the training and test data.\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # convert to float32 for efficiency\n",
    "# X_train = X_train.astype('float32')\n",
    "# X_test = X_test.astype('float32')\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# print(\"X_train shape:\", X_train.shape)\n",
    "# print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "# #  print(\"Unique values in y_train:\", set(y_train))\n",
    "\n",
    "# # Create a `Sequential` model\n",
    "# model = Sequential()\n",
    "\n",
    "# # Add several layers to your model. Make sure you use ReLU as the activation function for the middle layers. \n",
    "# # Use Softmax for the output layer because each output has a single lable and all the label probabilities add up to 1.\n",
    "\n",
    "# # model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))     # Input and first hidden layer\n",
    "# # model.add(Dense(32, activation='relu'))                                 # Second hidden layer\n",
    "# # model.add(Dense(16, activation='relu'))                                 # Third hidden layer\n",
    "# # model.add(Dense(len(set(y)), activation='softmax'))                      # Output layer with Softmax\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# # model.add(Dense(len(set(y)), activation='softmax'))\n",
    "# model.add(Dense(1, activation='linear'))  # Single neuron for regression\n",
    "\n",
    "\n",
    "# model.compile(optimizer='adam', \n",
    "#               loss='mean_squared_error', \n",
    "#               metrics=['mae'])\n",
    "\n",
    "\n",
    "# # Fit the training data.\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "# model.fit(X_train, y_train, epochs=30, batch_size=8, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "# # # Evaluate your neural network model with the test data.\n",
    "# test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# # Prepare data for tabulate\n",
    "# evaluation_data = [\n",
    "#     [\"Metric\", \"Value\"],\n",
    "#     [\"Test Accuracy\", f\"{test_accuracy:.2%}\"],  # Convert to percentage format\n",
    "#     [\"Test Loss\", f\"{test_loss:.4f}\"]\n",
    "# ]\n",
    "\n",
    "# # Print evaluation results as a table\n",
    "# print(tabulate(evaluation_data, headers=\"firstrow\", tablefmt=\"grid\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
